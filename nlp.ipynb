{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Beg/Ali-Beg/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vQ6P8HWwqNJ",
        "outputId": "ded8147b-ecae-4824-d934-81160c44485e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBbTT3lSv92J",
        "outputId": "c58dc99f-4b6b-4824-9532-1839338c7f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('quick', 1), ('brown', 1), ('littl', 1), ('fox', 1), ('jump', 1)]\n",
            "['Quick Brown little fox jumps over a littledog']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk. stem import PorterStemmer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "text = \"Quick Brown little fox jumps over a littledog\"\n",
        "tokens = word_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if\n",
        "word.lower() not in\n",
        "stop_words]\n",
        "porter = PorterStemmer()\n",
        "stemmed_tokens = [porter.stem(word) for word in filtered_tokens]\n",
        "fdist = FreqDist (stemmed_tokens)\n",
        "print(fdist.most_common(5))\n",
        "sentences = sent_tokenize(text)\n",
        "print (sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uhzrTh0PxDsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ph5rnvV5xDpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement morphological parser to accept and reject given string**"
      ],
      "metadata": {
        "id": "nhJJRPL8wXrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def morphological_parser(string) :\n",
        "# Morphological rules\n",
        "# Rule 1: String must start with a vowel\n",
        "  if re.match(r'^[aeiouAEIOU]', string):\n",
        "# Rule 2: String must end with a consonant\n",
        "    if re.match(r'.*[bcdfghjkImnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]$',string):\n",
        "# Rule 3: String must have at Least 4 characters\n",
        "      if len(string) >= 4:\n",
        "        return True\n",
        "      else:\n",
        "        return False, \"Rejected: String must have at least 4 character\"\n",
        "    else:\n",
        "      return False, \"Rejected: String must end with a consonant\"\n",
        "  else:\n",
        "    return False, \"Rejected: String must start with a vowel\"\n",
        "# Test the parser\n",
        "input_string = input(\"Enter string: \")\n",
        "accepted, reason = morphological_parser (input_string)\n",
        "if accepted:\n",
        "  print(\"Accepted!\")\n",
        "else:\n",
        "  print(reason)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNsy-eizwdQ5",
        "outputId": "a4b63655-ab39-4f03-e1ef-a4bbbfdc525a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter string:  an elder is always wise perso\n",
            "Rejected: String must start with a vowel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9gH82caxWss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement stemming and lemmatization for a corpus."
      ],
      "metadata": {
        "id": "BZyhdjKRx4KT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (punkt for sentence tokenization, wordnet for lemmatization)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"The quick brown foxes are jumping over the lazy dogs\",\n",
        "    \"I am happily studying natural language processing\",\n",
        "    \"The weather is quite pleasant today\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus (split sentences into words)\n",
        "tokenized_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "# Stemming (reduce words to their base form using PorterStemmer)\n",
        "porter = PorterStemmer()\n",
        "stemmed_corpus = [[porter.stem(word) for word in sentence] for sentence in tokenized_corpus]\n",
        "\n",
        "print(\"Stemmed Corpus:\")\n",
        "for sentence in stemmed_corpus:\n",
        "    print(sentence)  # Print each stemmed sentence\n",
        "\n",
        "# Lemmatization (reduce words to their dictionary form using WordNetLemmatizer)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in tokenized_corpus]\n",
        "\n",
        "print(\"Lemmatized Corpus:\")\n",
        "for sentence in lemmatized_corpus:\n",
        "    print(sentence)  # Print each lemmatized sentence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5dyhaTKx9pQ",
        "outputId": "c8d78f83-a490-4869-f2d3-c204d948de1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Corpus:\n",
            "['the', 'quick', 'brown', 'fox', 'are', 'jump', 'over', 'the', 'lazi', 'dog']\n",
            "['i', 'am', 'happili', 'studi', 'natur', 'languag', 'process']\n",
            "['the', 'weather', 'is', 'quit', 'pleasant', 'today']\n",
            "Lemmatized Corpus:\n",
            "['The', 'quick', 'brown', 'fox', 'are', 'jumping', 'over', 'the', 'lazy', 'dog']\n",
            "['I', 'am', 'happily', 'studying', 'natural', 'language', 'processing']\n",
            "['The', 'weather', 'is', 'quite', 'pleasant', 'today']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-3nBPL0yyh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform and analyse POS tagging -HMM"
      ],
      "metadata": {
        "id": "Bq9m7fmczAqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag import hmm\n",
        "\n",
        "# Download the Treebank corpus (ensure internet connection)\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Load the tagged sentences\n",
        "tagged_sentences = treebank.tagged_sents()\n",
        "\n",
        "# Split into training and testing sets (80% training, 20% testing)\n",
        "train_size = int(0.8 * len(tagged_sentences))\n",
        "train_sents = tagged_sentences[:train_size]\n",
        "test_sents = tagged_sentences[train_size:]\n",
        "\n",
        "# Train the HMM tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train(train_sents)\n",
        "\n",
        "# Evaluate the tagger's accuracy on the test set\n",
        "accuracy = tagger.evaluate(test_sents)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# New sentence for tagging\n",
        "new_sentence = \"The cat is sitting on the mat\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokenized_sentence = nltk.word_tokenize(new_sentence)\n",
        "\n",
        "# Tag the tokenized sentence using the trained HMM tagger\n",
        "tagged_sentence = tagger.tag(tokenized_sentence)\n",
        "\n",
        "# Print the tagged sentence\n",
        "print(\"Tagged Sentence:\", tagged_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FjpyLrOzBaG",
        "outputId": "2f31f4e8-018e-4023-a001-dbafe8cbe3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3647387594191327\n",
            "Tagged Sentence: [('The', 'DT'), ('cat', 'NNP'), ('is', 'NNP'), ('sitting', 'NNP'), ('on', 'NNP'), ('the', 'NNP'), ('mat', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvll9thZzGaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the viterbi algorithm Using python or NLTK"
      ],
      "metadata": {
        "id": "KtK08O7BzN_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def viterbi(observations, transition_matrix, emission_matrix):\n",
        "\n",
        "\n",
        "  num_hidden_states = transition_matrix.shape[0]\n",
        "  num_observations = len(observations)\n",
        "\n",
        "  # Initialize the Viterbi path probabilities and backpointers\n",
        "  viterbi_path = np.zeros((num_observations, num_hidden_states))\n",
        "  backpointers = np.zeros((num_observations, num_hidden_states), dtype=int)\n",
        "\n",
        "  # Initialization step (t=1)\n",
        "  for state in range(num_hidden_states):\n",
        "    viterbi_path[0, state] = emission_matrix[state, observations[0]]\n",
        "\n",
        "  # Forward pass (t=2 to T)\n",
        "  for t in range(1, num_observations):\n",
        "    for state in range(num_hidden_states):\n",
        "      max_prob = float('-inf')\n",
        "      for prev_state in range(num_hidden_states):\n",
        "        prev_prob = viterbi_path[t - 1, prev_state]\n",
        "        current_prob = prev_prob * transition_matrix[prev_state, state] * emission_matrix[state, observations[t]]\n",
        "        if current_prob > max_prob:\n",
        "          max_prob = current_prob\n",
        "          backpointers[t, state] = prev_state\n",
        "      viterbi_path[t, state] = max_prob\n",
        "\n",
        "  # Backtracking to find the most likely sequence\n",
        "  hidden_states = []\n",
        "  state = np.argmax(viterbi_path[-1, :])\n",
        "  for t in range(num_observations)[::-1]:\n",
        "    hidden_states.append(state)\n",
        "    state = backpointers[t, state]\n",
        "  hidden_states.reverse()\n",
        "\n",
        "  # Calculate the probability of the most likely sequence\n",
        "  best_path_prob = viterbi_path[-1, state]\n",
        "\n",
        "  return hidden_states, best_path_prob\n",
        "\n",
        "\n",
        "# Example usage (replace with your actual HMM parameters)\n",
        "transition_matrix = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
        "emission_matrix = np.array([[0.8, 0.2], [0.1, 0.9]])\n",
        "observations = [1, 0, 1]\n",
        "hidden_states, best_path_prob = viterbi(observations, transition_matrix, emission_matrix)\n",
        "\n",
        "print(\"Most likely sequence of hidden states:\", hidden_states)\n",
        "print(\"Probability of the most likely sequence:\", best_path_prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj1ihAGdzQLw",
        "outputId": "5d8c50ae-ed64-4b5d-af49-d04d1223ca97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most likely sequence of hidden states: [1, 0, 1]\n",
            "Probability of the most likely sequence: 0.04032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUC0S18Ezo3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 6 Implement a bigram model using 3 sentences in python orNLTK"
      ],
      "metadata": {
        "id": "qP0en-CmzpoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"I love natural language processing\",\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"NLTK is a powerful toolkit for natural language processing tasks\"\n",
        "]\n",
        "\n",
        "# Tokenize sentences into lowercase words\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Create bigrams for each sentence\n",
        "sentence_bigrams = [list(nltk.bigrams(tokens)) for tokens in tokenized_sentences]\n",
        "\n",
        "# Build bigram model (dictionary with nested dictionaries)\n",
        "bigram_model = {}\n",
        "for sentence_bigram in sentence_bigrams:\n",
        "    for bigram in sentence_bigram:\n",
        "        # First word in the bigram (key)\n",
        "        first_word = bigram[0]\n",
        "\n",
        "        # Check if the first word exists as a key in the bigram_model\n",
        "        if first_word not in bigram_model:\n",
        "            bigram_model[first_word] = {}  # Create an empty inner dictionary\n",
        "\n",
        "        # Second word in the bigram (value) and its count\n",
        "        second_word = bigram[1]\n",
        "        count = bigram_model[first_word].get(second_word, 0)  # Get existing count or default to 0\n",
        "        bigram_model[first_word][second_word] = count + 1  # Update or create new count\n",
        "\n",
        "# Print the bigram model\n",
        "for word in bigram_model:\n",
        "    print(word, \":\", bigram_model[word])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpxdgt6Gzvbe",
        "outputId": "4b94daa3-1419-4441-b65d-0a4a43b51b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i : {'love': 1}\n",
            "love : {'natural': 1}\n",
            "natural : {'language': 2}\n",
            "language : {'processing': 2}\n",
            "the : {'quick': 1, 'lazy': 1}\n",
            "quick : {'brown': 1}\n",
            "brown : {'fox': 1}\n",
            "fox : {'jumps': 1}\n",
            "jumps : {'over': 1}\n",
            "over : {'the': 1}\n",
            "lazy : {'dog': 1}\n",
            "nltk : {'is': 1}\n",
            "is : {'a': 1}\n",
            "a : {'powerful': 1}\n",
            "powerful : {'toolkit': 1}\n",
            "toolkit : {'for': 1}\n",
            "for : {'natural': 1}\n",
            "processing : {'tasks': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUAPajn70ECv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMldNLwo0H_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lab 7 Text classification using NaÃ¯ve Bayes Classifier"
      ],
      "metadata": {
        "id": "U8EvaOnl0Hz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Sample data (replace with your actual dataset)\n",
        "corpus = [\n",
        "    (\"This is a good movie\", \"positive\"),\n",
        "    (\"I enjoyed this movie a lot\", \"positive\"),\n",
        "    (\"This movie is fantastic\", \"positive\"),\n",
        "    (\"What a waste of time\", \"negative\"),\n",
        "    (\"I wouldn't recommend this movie\", \"negative\"),\n",
        "    (\"Terrible acting\", \"negative\")\n",
        "]\n",
        "\n",
        "# Split data into features (X) and labels (y)\n",
        "X, y = zip(*corpus)  # Unpack tuples into separate lists of features and labels\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)  # Set random_state for reproducibility\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS7FgTQa0JWn",
        "outputId": "c27bf210-62e9-4885-dfa4-5470cc493049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00         0\n",
            "    positive       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.50      0.25      0.33         2\n",
            "weighted avg       1.00      0.50      0.67         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8-5rmKT0cFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 8 Sentiment analysis using SVM"
      ],
      "metadata": {
        "id": "8cKWXxq80bo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Sample data (replace with your actual dataset)\n",
        "corpus = [\n",
        "    (\"This is a good movie\", \"positive\"),\n",
        "    (\"I enjoyed this movie a lot\", \"positive\"),\n",
        "    (\"This movie is fantastic\", \"positive\"),\n",
        "    (\"What a waste of time\", \"negative\"),\n",
        "    (\"I wouldn't recommend this movie\", \"negative\"),\n",
        "    (\"Terrible acting\", \"negative\")\n",
        "]\n",
        "\n",
        "# Split data into features (X) and labels (y)\n",
        "X, y = zip(*corpus)  # Unpack tuples into separate lists\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)  # Set random_state for reproducibility\n",
        "\n",
        "# Train SVM classifier with linear kernel\n",
        "classifier = SVC(kernel='linear')  # You can experiment with different kernels (e.g., 'rbf', 'poly')\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-IJIdTm0g5u",
        "outputId": "07bbc573-88e9-4aa8-bcef-d84c38518ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       0.0\n",
            "    positive       0.00      0.00      0.00       2.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_yJKlt21CaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design of ANN for email spam classification"
      ],
      "metadata": {
        "id": "JuMpSt4r1Rqe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "blGhy3bv1SH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ot1vi4tL1XCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-dCZDry2AT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fTZ6_QU2RSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEtrnfPG2dQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample data (replace with your actual dataset)\n",
        "emails = [\n",
        "    \"This is a good movie\",\n",
        "    \"I enjoyed this movie a lot\",\n",
        "    \"This movie is fantastic\",\n",
        "    \"What a waste of time\",\n",
        "    \"I wouldn't recommend this movie\",\n",
        "    \"Terrible acting\"\n",
        "]\n",
        "labels = [0, 0, 0, 1, 1, 1]  # 0 for positive, 1 for negative\n",
        "\n",
        "# Preprocess the text data\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    words = text.split()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "emails = [preprocess_text(email) for email in emails]\n",
        "\n",
        "# Convert emails into numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "y = np.array(labels)  # Convert labels to a NumPy array\n",
        "\n",
        "# Split data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert sparse matrices to dense format for compatibility with Keras\n",
        "X_train_dense = X_train.toarray()\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train_dense.shape[1],)),  # Input layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),  # Dropout layer with 50% dropout rate to prevent overfitting\n",
        "    Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation\n",
        "    Dropout(0.5),  # Dropout layer with 50% dropout rate\n",
        "    Dense(32, activation='relu'),  # Hidden layer with 32 neurons and ReLU activation\n",
        "    Dropout(0.5),  # Dropout layer with 50% dropout rate\n",
        "    Dense(1, activation='sigmoid')  # Output layer with 1 neuron and sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_dense, y_train, epochs=20, batch_size=16, verbose=1)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "y_pred = (model.predict(X_test_dense) > 0.5).astype(\"int32\")  # Threshold predictions to 0 or 1 based on a probability cutoff of 0.5\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTX2FBM72mjU",
        "outputId": "05444422-8ee3-45c7-a559-c82a825b1af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6876 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7918 - accuracy: 0.2500\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7066 - accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7128 - accuracy: 0.2500\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6493 - accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6674 - accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7072 - accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6358 - accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6233 - accuracy: 0.7500\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7066 - accuracy: 0.2500\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6160 - accuracy: 0.7500\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6519 - accuracy: 0.7500\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7026 - accuracy: 0.7500\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6505 - accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6222 - accuracy: 0.7500\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6283 - accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6820 - accuracy: 0.7500\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5567 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6324 - accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6880 - accuracy: 0.5000\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-F7NNgpH21XX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}